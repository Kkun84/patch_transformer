{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "patch_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfLUPXAAFYDJi/SNE2hVwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kkun84/patch_transformer/blob/master/colab/patch_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqHOz5CP1Pqo",
        "outputId": "b9ded7ef-254c-4982-e660-2dfd1a3dd9f8"
      },
      "source": [
        "!pip install torchinfo"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (0.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkWOk4f6MLJk"
      },
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch import tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        image_size: int = 28,\n",
        "        image_channels: int = 1,\n",
        "        patch_size: int = 7,\n",
        "        num_classes: int = 10,\n",
        "        dim: int = 64,\n",
        "        nhead: int = 1,\n",
        "        dim_feedforward: int = 64,\n",
        "        depth: int = 3,\n",
        "        dropout: float = 0.5,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert (\n",
        "            image_size % patch_size == 0\n",
        "        ), f'{image_size}, {patch_size}, {image_size % patch_size}'\n",
        "\n",
        "        self.input_shape = (image_channels, image_size, image_size)\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.image_channels = image_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.dim = dim\n",
        "\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_dim = image_channels * patch_size ** 2\n",
        "\n",
        "        # self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "\n",
        "        self.patch_embedding = nn.Linear(self.patch_dim, dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer=encoder_layer, num_layers=depth\n",
        "        )\n",
        "\n",
        "        self.mlp_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def unfold_patch(self, image: Tensor) -> Tensor:\n",
        "        batch_size = len(image)\n",
        "\n",
        "        verify_shape = torch.Size(\n",
        "            [batch_size, self.image_channels, self.image_size, self.image_size]\n",
        "        )\n",
        "        assert image.shape == verify_shape, f'{image.shape}, {verify_shape}'\n",
        "\n",
        "        x = image.unfold(2, self.patch_size, self.patch_size).unfold(\n",
        "            3, self.patch_size, self.patch_size\n",
        "        )\n",
        "\n",
        "        verify_shape = torch.Size(\n",
        "            [\n",
        "                batch_size,\n",
        "                self.image_channels,\n",
        "                self.image_size // self.patch_size,\n",
        "                self.image_size // self.patch_size,\n",
        "                self.patch_size,\n",
        "                self.patch_size,\n",
        "            ]\n",
        "        )\n",
        "        assert x.shape == verify_shape, f'{x.shape}, {verify_shape}'\n",
        "\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).reshape(\n",
        "            batch_size,\n",
        "            self.num_patches,\n",
        "            self.image_channels,\n",
        "            self.patch_size,\n",
        "            self.patch_size,\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    def forward(self, image: Tensor) -> Tensor:\n",
        "        batch_size = len(image)\n",
        "\n",
        "        patches = self.unfold_patch(image)\n",
        "        x = patches.flatten(2)\n",
        "\n",
        "        verify_shape = torch.Size([batch_size, self.num_patches, self.patch_dim])\n",
        "        assert x.shape == verify_shape, f'{x.shape}, {verify_shape}'\n",
        "\n",
        "        x = self.patch_embedding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(1)\n",
        "\n",
        "        verify_shape = torch.Size([batch_size, self.dim])\n",
        "        assert x.shape == verify_shape, f'{x.shape}, {verify_shape}'\n",
        "\n",
        "        x = self.mlp_head(x)\n",
        "        return x"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb5eukNq1KpD",
        "outputId": "a4783f41-6aa6-4a1a-e616-8c332d7a39ea"
      },
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchinfo import summary\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "lr = 0.001\n",
        "max_epoch = 30\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "writer = SummaryWriter()\n",
        "log_dir = Path(writer.get_logdir())\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "model = TransformerModel().to(device)\n",
        "print(summary(model, (2, *model.input_shape)))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "n_iter = 0"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===============================================================================================\n",
            "Layer (type:depth-idx)                        Output Shape              Param #\n",
            "===============================================================================================\n",
            "TransformerModel                              --                        --\n",
            "├─Linear: 1-1                                 [2, 16, 64]               3,200\n",
            "├─TransformerEncoder: 1-2                     [2, 16, 64]               --\n",
            "│    └─ModuleList: 2                          --                        --\n",
            "│    │    └─TransformerEncoderLayer: 3-1      [2, 16, 64]               25,216\n",
            "│    │    └─TransformerEncoderLayer: 3-2      [2, 16, 64]               25,216\n",
            "│    │    └─TransformerEncoderLayer: 3-3      [2, 16, 64]               25,216\n",
            "├─Linear: 1-3                                 [2, 10]                   650\n",
            "===============================================================================================\n",
            "Total params: 79,498\n",
            "Trainable params: 79,498\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.06\n",
            "===============================================================================================\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.21\n",
            "Params size (MB): 0.32\n",
            "Estimated Total Size (MB): 0.54\n",
            "===============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtgnL73617jp",
        "outputId": "6b177b5f-7004-4c94-e972-af9a93da559a"
      },
      "source": [
        "for epoch in range(max_epoch):\n",
        "    model.train()\n",
        "    for i, (images, labels) in tqdm(\n",
        "        enumerate(train_dataloader),\n",
        "        desc=f'Train {epoch}/{max_epoch-1}',\n",
        "        total=len(train_dataloader),\n",
        "    ):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = F.cross_entropy(outputs, labels, reduction='mean')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = (outputs.max(1)[1] == labels).float().mean().item()\n",
        "\n",
        "        writer.add_scalar('metrics/train_loss', loss.item(), n_iter)\n",
        "        writer.add_scalar('metrics/train_accuracy', accuracy, n_iter)\n",
        "\n",
        "        n_iter += 1\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for (images, labels) in tqdm(\n",
        "            test_dataloader,\n",
        "            desc=f'Test {epoch}/{max_epoch-1}',\n",
        "            total=len(test_dataloader),\n",
        "        ):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss += F.cross_entropy(outputs, labels, reduction='sum').item()\n",
        "            correct += (outputs.max(1)[1] == labels).sum().item()\n",
        "            total += len(labels)\n",
        "\n",
        "        loss /= total\n",
        "        accuracy = correct / total\n",
        "\n",
        "        writer.add_scalar('metrics/test_loss', loss, n_iter)\n",
        "        writer.add_scalar('metrics/test_accuracy', accuracy, n_iter)\n",
        "\n",
        "    torch.save(model.state_dict(), log_dir / f'epoch{epoch:05}.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train 0/29: 100%|██████████| 938/938 [00:13<00:00, 68.75it/s]\n",
            "Test 0/29: 100%|██████████| 157/157 [00:01<00:00, 129.01it/s]\n",
            "Train 1/29: 100%|██████████| 938/938 [00:13<00:00, 68.61it/s]\n",
            "Test 1/29: 100%|██████████| 157/157 [00:01<00:00, 126.04it/s]\n",
            "Train 2/29: 100%|██████████| 938/938 [00:13<00:00, 67.87it/s]\n",
            "Test 2/29: 100%|██████████| 157/157 [00:01<00:00, 124.99it/s]\n",
            "Train 3/29: 100%|██████████| 938/938 [00:13<00:00, 68.18it/s]\n",
            "Test 3/29: 100%|██████████| 157/157 [00:01<00:00, 126.34it/s]\n",
            "Train 4/29: 100%|██████████| 938/938 [00:13<00:00, 68.32it/s]\n",
            "Test 4/29: 100%|██████████| 157/157 [00:01<00:00, 127.42it/s]\n",
            "Train 5/29: 100%|██████████| 938/938 [00:13<00:00, 67.96it/s]\n",
            "Test 5/29: 100%|██████████| 157/157 [00:01<00:00, 126.04it/s]\n",
            "Train 6/29: 100%|██████████| 938/938 [00:13<00:00, 68.83it/s]\n",
            "Test 6/29: 100%|██████████| 157/157 [00:01<00:00, 123.32it/s]\n",
            "Train 7/29: 100%|██████████| 938/938 [00:13<00:00, 67.98it/s]\n",
            "Test 7/29: 100%|██████████| 157/157 [00:01<00:00, 126.12it/s]\n",
            "Train 8/29: 100%|██████████| 938/938 [00:13<00:00, 67.77it/s]\n",
            "Test 8/29: 100%|██████████| 157/157 [00:01<00:00, 126.64it/s]\n",
            "Train 9/29: 100%|██████████| 938/938 [00:13<00:00, 67.91it/s]\n",
            "Test 9/29: 100%|██████████| 157/157 [00:01<00:00, 126.52it/s]\n",
            "Train 10/29: 100%|██████████| 938/938 [00:13<00:00, 68.12it/s]\n",
            "Test 10/29: 100%|██████████| 157/157 [00:01<00:00, 123.24it/s]\n",
            "Train 11/29: 100%|██████████| 938/938 [00:13<00:00, 68.00it/s]\n",
            "Test 11/29: 100%|██████████| 157/157 [00:01<00:00, 125.54it/s]\n",
            "Train 12/29:  22%|██▏       | 206/938 [00:03<00:10, 69.45it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbC9uhwB2EYH"
      },
      "source": [
        "for index in range(20):\n",
        "    print(index)\n",
        "    image, label = test_dataset[index]\n",
        "\n",
        "    display(to_pil_image(image))\n",
        "\n",
        "    patches = model.unfold_patch(image[None])[0]\n",
        "    display(to_pil_image(make_grid(patches, nrow=model.num_patches, pad_value=0.5)))\n",
        "\n",
        "    output_prob = model(image[None])[0].softmax(0)\n",
        "\n",
        "    print(output_prob.tolist())\n",
        "\n",
        "    print(output_prob.argmax())\n",
        "\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np4CmP-a1WSk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}