{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "attentionの可視化をするコードを追記する予定です"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_dir_ = 'runs/Jun03_14-52-17_99986f60c588'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "target_model_dir = Path('..', target_model_dir_)\n",
    "target_model_dir = target_model_dir.resolve()\n",
    "assert target_model_dir.is_dir(), target_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path = str(target_model_dir / 'src')\n",
    "if path not in sys.path:\n",
    "    sys.path.insert(0, path)\n",
    "\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchinfo import summary\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerModel                              --                        --\n",
       "├─Linear: 1-1                                 [2, 16, 64]               3,200\n",
       "├─TransformerEncoder: 1-2                     [2, 16, 64]               --\n",
       "│    └─ModuleList: 2                          --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [2, 16, 64]               25,216\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [2, 16, 64]               25,216\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [2, 16, 64]               25,216\n",
       "├─Linear: 1-3                                 [2, 10]                   650\n",
       "===============================================================================================\n",
       "Total params: 79,498\n",
       "Trainable params: 79,498\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.06\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.21\n",
       "Params size (MB): 0.32\n",
       "Estimated Total Size (MB): 0.54\n",
       "==============================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "from model.model import TransformerModel\n",
    "\n",
    "model = TransformerModel()\n",
    "model.eval()\n",
    "\n",
    "summary(model, (2, *model.input_shape), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/workspace/runs/Jun03_14-52-17_99986f60c588/epoch00029.pt\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "weight_path = sorted(target_model_dir.glob('*.pt'))[-1]\n",
    "print(weight_path)\n",
    "\n",
    "model.load_state_dict(torch.load(weight_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = MNIST(root='../data', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28 at 0x7F1F2CB5A5E0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=146x11 at 0x7F1F2CB5A1F0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAAALCAIAAAAbcENSAAACIklEQVR4nO1XsesxcRh/fudnUBZEWchiYLpSLk5ZlBgMDP4BDDIoRl0YlMFgsBuUnYS6KwaWWyQcYSEpRWEQyjt8S28/74Lr91I+yz3P5/ne5/vU55773n1RFAUfvBuw/93AB4/gG13i8fhtDQ3iL5Q8Ho9erwcAk8m0WCwOh0OhUFgul+Px+Dfb+FEym80MwwiFQqvV2mw2edxLq9VyHIdIiqJcLlepVLpL8Pu28PtIp9P5fB4AbDYbYgKBwG636/V6AFCr1QCg1Wr9uGs+n6MFBoOBZVneu3I4HEKhkKbpdrvNo6xara7X69c0Go2Wy+V7RV7CNp/PR5IkAEQikX6/r9PpcBy3Wq0EQcxmM7SGIAgUnM/n1WqlVCoBANnm9Xp5t00kEtnt9uPxSFHU6XTiUdnv96tUqmvaaDQul8u9Ii9hG03TyLZMJgMA1WoVACQSCY7jLMuGw2H4axAPh8NoNBoMBlKpFDHT6ZT3lqLRKI7j1Wr1dsqfgcViCYVCz+u8hG3/xGazYRjmmtI0fY3dbrdEIul2uygtFov8bu10OmOx2Ha7TSaT/CqTJCkWiwFgMpkgZr/fP6Dzfl+SCoUil8thGJZIJBCzXq95UZbJZCjIZrMCgaBSqfB7ql3R6XSMRiOKOY57QOH9bAsGg3K5fLPZDIdDHmUFAgF6OQOARqOZTCaxWIxHfYRUKoVhGI7jTz5qX5/f7XfE+03bBwDwB/Hd8qRXxmvMAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.004493934102356434, 0.05076932907104492, 0.018564721569418907, 0.002767853671684861, 0.01326053962111473, 0.014602843672037125, 0.00039116654079407454, 0.799828827381134, 0.0628809854388237, 0.03243980184197426]\ntensor(7)\n\n1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28 at 0x7F1F2CB5A5E0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=146x11 at 0x7F1F2CB5A1F0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAAALCAIAAAAbcENSAAACcUlEQVR4nO2XP2jqYBDAr89GcRJsQTqrXRwEiRScgy6aYhFd2tnFpY3WWmzEUh2cOjk6uJVAkDo5tYJoBZFSkDq2XaISClpxidYOH4Rg3x9f+Xx9gr8ld9/lu7vkuPuStUQiASuWjR/fncCKr7COLslk8rMNNeK/NDWbTZ7nHx8fXS6XIAhKkyRJLMsSBEFR1M3NDcY0qtXqzs4OAFxeXg4GgwU9Ms/zu7u7StP5+TkAsCw7s2U8HqfTaQA4OTkBgOvr60ajAQCVSqVWqyGH658jfSPFYtFkMr29vb2+vs6YAoEAQRDYI9I0bbPZAOD29jaRSDAMgz0EYm9v7/j4WH4Ei8Uyc0Mul3t6egIAnuf9fj8AaLXaX3n7v8oGAM/Pz0o1EokgYXt7GwDq9Xq9XscVa2Nj4/T0FL3K+/v74XCIy/NPyWQyShX1jUql+oKrvzjbwuHw2dkZx3HT6RStTKfTbDZ7cHDwhcDz4Ha70SQBALVa3ev1YrHYaDTC5Z9hGLvdDgCFQmG5Ps3m7barqyufz4fk9/d3WQgGgxRFlcvll5cX7MmRJKlWq5U5lMtljP6Pjo6QEAqFFt1qeJmrbMqatdvtUqmE5GKx6PF4jEbj/v4+OkUxUigUnE6nrObz+Xg8jjeEjF6vlyRJVjc3NwGg3+9LkkQQhE6nk01ms/nw8BDJk8kkGo1i7P75+XPZSJL0er0A0Gq1aJoWRXE4HKKR4vf77+7urFarXq/Hm9bW1pbD4dBoNKIoopWLi4vFNcTDwwMS0EzudrsAwHGcIAgGgyEQCMimdrut3NjpdFKp1IKy+g1ryzXTVyBWv9tLyQcjKg+PHq2MgwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.0003057601279579103, 0.004064821172505617, 0.3416615426540375, 0.03231873735785484, 0.00042588802170939744, 0.503757119178772, 0.0001153868215624243, 0.11243999749422073, 0.0040403869934380054, 0.0008704166393727064]\ntensor(5)\n\n"
     ]
    }
   ],
   "source": [
    "for index in range(2):\n",
    "    print(index)\n",
    "    image, label = dataset[index]\n",
    "\n",
    "    display(to_pil_image(image))\n",
    "\n",
    "    patches = model.unfold_patch(image[None])[0]\n",
    "    display(to_pil_image(make_grid(patches, nrow=model.num_patches, pad_value=0.5)))\n",
    "\n",
    "    output_prob = model(image[None])[0].softmax(0)\n",
    "\n",
    "    print(output_prob.tolist())\n",
    "\n",
    "    print(output_prob.argmax())\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 10]),\n",
       " [0.0003057601279579103,\n",
       "  0.004064821172505617,\n",
       "  0.3416615426540375,\n",
       "  0.03231873735785484,\n",
       "  0.00042588802170939744,\n",
       "  0.503757119178772,\n",
       "  0.0001153868215624243,\n",
       "  0.11243999749422073,\n",
       "  0.0040403869934380054,\n",
       "  0.0008704166393727064])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "x = image[None]\n",
    "x = model.unfold_patch(x)\n",
    "x = x.flatten(2)\n",
    "x = model.patch_embedding(x)\n",
    "x = model.transformer_encoder(x)\n",
    "x = x.mean(1)\n",
    "x = model.mlp_head(x)\n",
    "x.shape, x[0].softmax(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (1): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (2): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "list(model.transformer_encoder.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (patch_embedding): Linear(in_features=49, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}